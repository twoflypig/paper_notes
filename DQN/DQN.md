# Why DQN

若使用表格來存储每个状态，和在当前state下每个action所拥有的Q值，那么对于复杂问题下，状态太多。故我们可以使用神经网络来代替该表。将状态和动作当做神经网络的输入，然后经过神经网络的分析得到该动作的Q值。另一种做法是只输入状态值, 输出所有的动作值, 然后按照 Q learning 的原则, 直接选择拥有最大值的动作当做下一步要做的动作。在本次中使用第二种

# 两大利器

- Experience replay.将过去的经历保存下来，当做训练库。
- Fixed Q-targets.这样就会存在两个神经网络