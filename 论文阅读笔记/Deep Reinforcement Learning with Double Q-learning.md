# Abstract

流行的Q-learning 算法在某些特定情况下会高估了action的values。在本文中，我们回答了这个情况会造成的影响。特别的是，我们先展示最近的DQN算法在一些Atari2600游戏中发生明显的高估，然后我们将以表格的形式阐明在Double-DQN后面的想法，而且这个idea能够泛化到大量级的函数近似中去。我们提出了DQN的一个特别改进，结果显示算法的结果不仅减少了观察到的高估，并且帮助在几个游戏中产生更好的结果。

# Introduction

增强学习的目标就是去学习在序列决策问题中的好的策略，通过最优化未来累积的reward信号。Q-learning是最受环境的RF算法之一，但是有时算法又会对一些action value给出过大的估计因为它是在估计action values上使用了最大化估计，这就可能会高估了一些值

本文中，结合前人的思想，我们发现当action values是近似误差的源的不精确和不相关的时候高估就会发生。当然了，不精确的值的估计在学习过程中是正常的，这就指出了高估会比之前发现的还是普遍。

高估如果发生了，在实际中会消极的影响表现吗？过于乐观的值估计在问题中是可有可无的。如果所有的值估计统一的更高那么相关的行为偏好会被保留并且我们不会期望得到的政策会更糟。更进一步，有时乐观还是不错的。在不确定性下乐观也是一个探索的技术。然而，如何高估是不统一的并且不是集中在我们期望去学习的状态，那么就可能会负面的影响结果的策略。 

我们的Double-DQN改善了结果，这就证明了高估的确会产生坏影响

