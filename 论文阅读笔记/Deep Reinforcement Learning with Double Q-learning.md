# Abstract

流行的Q-learning 算法在某些特定情况下会高估了action的values。在本文中，我们回答了这个情况会造成的影响。特别的是，我们先展示最近的DQN算法在一些Atari2600游戏中发生明显的高估，然后我们将以表格的形式阐明在Double-DQN后面的想法，而且这个idea能够泛化到大量级的函数近似中去。我们提出了DQN的一个特别改进，结果显示算法的结果不仅减少了观察到的高估，并且帮助在几个游戏中产生更好的结果。

# Introduction

增强学习的目标就是去学习在序列决策问题中的好的策略，通过最优化未来累积的reward信号。Q-learning是最受环境的RF算法之一，但是有时算法又会对一些action value给出过大的估计因为它是在估计action values上使用了最大化估计，这就可能会高估了一些值

本文中，结合前人的思想，我们发现当action values是近似误差的源的不精确和不相关的时候高估就会发生。当然了，不精确的值的估计在学习过程中是正常的，这就指出了高估会比之前发现的还是普遍。

高估如果发生了，在实际中会消极的影响表现吗？过于乐观的值估计在问题中是可有可无的。如果所有的值估计统一的更高那么相关的行为偏好会被保留并且我们不会期望得到的政策会更糟。更进一步，有时乐观还是不错的。在不确定性下乐观也是一个探索的技术。然而，如何高估是不统一的并且不是集中在我们期望去学习的状态，那么就可能会负面的影响结果的策略。 

我们的Double-DQN改善了结果，这就证明了高估的确会产生坏影响

# Background

为了解决序列决策问题我们能够学习对每个行为的最优值的估计，这个估计被定义为当我们采用那个行为并且此后保持着最优的策略时，未来回报的期望和。给定策略 $\pi$ ,在状态s中行为a的真实值为  (公式，意思就是此后的带有衰减的reward之和的期望)。最优的值就是在(在s,a下最大的Q值)。一个最优的策略很容易通过在每个状态选择最大的值来得到。

讲了下标准的Q-learning的参数更新，跟DQN差不多

# Deep Q Networks

介绍了DQN中的两个思想:target network和experience replay

# Double Q-learing

在标准Q-learing和DQN中对值取最大值的做法，会使得高估的可能性增大，就会产生过于乐观的值估计。**为了解决这个问题，我们decouple the selcetion from the evaluation** (对评估中的选择进行解耦合？)

在原始DQN中，两个值函数是通过assign each experinece randomly来更新两个值函数中的一个，这样就会有两个权值的集和，$\theta$ 和 $\theta^-$ 。对于每次更新，权值中的一个集合被用来去决定贪婪的策略并且另一个去决定策略背后的值。为了让对比明显，文中给出了DQN和doubleDQN的目标数学表达(balabala,不太理解这个区别)。注意到在argmax中行为的选择还是根据在线权值$\theta_i$ 。这就意味在Q-learning中，我们还是根据当前的值，定义为 $\theta_t$来估计贪婪策略的值。然而我们使用 $\theta$ ‘ 来公平的评估这个策略的值。第二个权值的集合能够被同步的更新通过switch两个$\theta$的角色(就是说再参加一个参数来评估好坏？)

# 由于估计误差的overoptimisim

Q-learning的过度估计，如果行为的值包含在区间[-e,e]之间的正太随机误差，那么每个target就会被高估到一个值，其中m是动作的数量。另外，两个作者给出了一个具体的例子，例子中这些高估会渐进的导致一个次优的策略，并且展示了当使用函数近似时，高估会在小的问题上很明显?后来由一个作者认为在环境中的噪声能够导致高估即使使用了表格表达，并且提出了double-Q-learning作为解决方案

在这一节我们证明了任何种类的估计误差能够导致一个上升的误差，无视是否这些误差是因为环境噪声，函数近似，非平稳性，或者其他来源噪声的。这一点很重要，因为实际中任何方法都会在学习中因为最初不知道真实的值而导致一些不精确。

又两位作者对一个特定的设置能够给出高估的上界，但是也有可能去得到一个更低的界限。

讲了一个理论:就是说估计值在1次上误差和是0，但是在二次上误差和上就是一个常数。同样条件下ddqn的误差下界是0

注意我们不需要去假设不同行为的估计误差是无关的。上面这个理论展示了即使值估计在平均水平上是正确的，任何源的估计误差会导致估计变大并且偏离真实的最优值

动作越多，高估就越明显的(看到了表二)

# Double DQN

Double Q-learning的想法是通过在target中取最大的操作分解到行为的选择和行为的评估中。尽管并不是完全的解耦，但是在DQN中的目标网络自然的充当了第二个值函数的候选者，而不用去引用额外的网络。我们因此提出去根据在线网络去评估贪婪策略，使用目标网络去估计贪婪策略的值。参考DQlearning和DQN，我们引出最终的算法Double-DQN.它和DQN一样更新，但是用一个(式子)去代替目标。

与DQN进行比较，第二个网络的权值被用来评估当前贪婪策略的目标网络的权值代替。目标网络的更新从DQN中会保持不变，并且会保持对在线网络的周期复制参数。

# note

核心是把target Q中的选择和评估动作分离，即选择最大的Q值时使用一个网络，然后评估这个最大Q值对应的动作时使用另一个网络。

过程如下

- Current Q-network w is used to select actions 
- Older Q-network  **$w^-$** is used to elvaluate actions  

与DQN中的不同，double-DQN是先在$S_{t+1}$ 的状态下再选择一个动作，再放在target中评估。而DQN在$S_{t}$ 的情况下选择一个动作就给target输入$S_{t+1}$ 来评估这个动作

