# Abstract

第一个从深度模型，使用卷积神经网络，把图像的原始像素作为输入，用神经网络作为Q值函数的估计

# Introduction

深度学习的突破，利用了很多结构例如convolutional networks, multilayer perceptrons, restricted Boltzmann machines and recurrent neural networks 。几个比较多大的挑战。第一:大多数成功的深度神经网络要求大量标记的训练数据。但是增强学习必须从数值型的reward上(经常是稀疏的，带有噪声和延迟的)。在行为和最终的结果之间，可能会有几千个timestep长的延迟，和在监督学习中的输入和目标之间的直接关联相比，这看起来就特别的令人畏惧。另一个问题就是增强学习遇到的是**高度相关** 的序列(而深度学习假设数据样本之间是无关的)。而且，在RL中随着算法的改变数据的分布也会发生改变(而深度学习假设的是一个固定的分布)

本文证明了卷积神经网络能够克服这些挑战从原始的录像数据中学习到成功的控制策略。训练算法是Q-learning算法的辩题，用SGD去更新参数。针对数据的关联和非固定分布问题，我们使用经验回放(随机从之前的转换中采样)，因此平滑了过去行为的训练分布。

# Background

注意通常上游戏的分数依赖于整个之前的动作和观察序列;关于一个行为的反馈可能会在几千步以后才会得到。

单独靠屏幕的输入是很难理解当前的状态的，因此我们还考虑了行为和观察构成的序列，假定是有限的。那么就可以用MDP的方法去做。

机器人的目标就是选择行为去最大化未来的回报。我们假设未来的回报会因为每一步而得到衰减$\gamma$ ，并且定义了在t时间时未来打过折扣的回报为$R_T$ = $\sum_{t^{‘}=t}^{T} \gamma^{t^{'}-t} r_t^{'} \qquad $,其中T是游戏终止的时间。我们定义最佳的行为-值函数 $Q^{*} (s,a)$ 作为执行某个策略后的最大的期望返回,after一些序列以后然后执行动作a,

 $Q^{*} (s,a)$  = $max_\pi E[R_t | s_t = s ,a_t =a,\pi ]$ ,其中$\pi$ 是一个把序列映射到行为的策略，

最优的action-value同时还有一个重要的身份 *Bellamn equation* 。这是基于这样的直觉: 如果在下一个time-step $s^{’}$ 的最优 $Q^{*} (s^{'},a^{'})$  是对所有的actions $a^{‘}$ 是可知的，那么最优的策略就是去选择行为 $a^{'}$ 来最大化 r+$\gamma Q^{*}(s^{'},a^{’})$  的期望值

 $Q^{*} (s,a)$  = $E_{s^{'} - \varepsilon}  [R_t | s_t = s ,a_t =a,\pi ]$   

很对增强学习算法背后的思想是去估计action-value的函数，通过使用Bellman equation作为迭代更新, $Q_{i+1} (s,a)$  = $E[r+ \gamma Q^{*}(s^{'},a^{’}]$ . 这样 值迭代算法 会收敛到最优的值函数中，在i$\to \propto $ 时 $Q_I \to Q^{*} $ 。在实际中，这个基本的方法是不适用的，因为action-value函数是对于每个序列分别的估计，没有任何的通用性。相反的，大家常使用函数近似来估计action-value函数,$Q(s,a;\theta) = Q^{*} (s,a)$ . 社区中经常使用线性函数近似，有时也用非线性函数近似，例如神经网络。我们使用一个带有权值参数$\theta$ 的神经网络作为近似函数，as Q-network .  Q-network能够在每次迭代的情况下最小化序列的损失 $L_i (\theta_i)$ 来训练

$ L_i (\theta_i)  = E_{s,a-p(\cdot)}[ ( y_i -Q(s,a;\theta_i)  )^2]$  

其中$y_i$ = $E_{s^{‘} - \varepsilon}[r+ \gamma max_{a^{’}}Q^{*} (s^{'},a^{’},\theta_{i-1})|s,a]$ 是迭代i的目标并且$\rho(s,a)$ 对序列s和行为a的可能分布(我们成为行为分布).来自之前$\theta_{i-1}$的 参数是固定的当我们在最小化损失函数 $L_i(\theta_i)$ 。注意目标依赖神经网络的权值；这和监督学习中目标的用法相反(在学习开始前是固定的)。通过对权值进行偏微分我们可以得到下式

(3)

不是对所有的样本进行迭代，我们使用SGD

# Related Work

TD-gammon不具有广泛性

# Deep Reinforcement Learning

