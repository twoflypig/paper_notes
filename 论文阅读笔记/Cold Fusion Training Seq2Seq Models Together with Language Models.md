# Abstract

在语言模型中，利用未标记的数据可以进一步提高模型的表现。本文提出一个方法，即在训练过程中利用预先训练好的语言模型。带有这个方法的模型能够更快的收敛并且有更好的泛化性能，并且能够以原来标记数据的10%来迁移到新的领域中去。

# 1 Introduction

Seq2Seq效果好，但是进一步提高生成文本的流畅度的话需要更多的数据but it is often augmented with a language model to further improve the fluency of the generated text.)

因为语言模型可以从大量的无监督的文本预料中进行训练，利用标记的领域中的文法信息可以相当的提高S2S的表现。一个标准的方式去整合语言模型就是线性的结合带有辅助语言模型的S2S模型来指导定向搜索(beam search)。 提出了一个重要算法叫做Deep Fusion 。可以融合S2S解码器中隐藏层的状态和一个带有门机制的神经语言模型，在两个模型独立的训练之后。

这种方法比baseline好，但是有一些局限。首先，S2S模型是训练来产生完整的标记序列without语言模型，它的解码器从训练标记中学习一个隐藏的语言模型，在解码器容量中占据一个有意义的部分来学习丰富的信息。第二，带有残差语言模型的s2s解码器偏向相似文本的训练标记。例如，如果s2s完全在一个合法的文件上训练然后与一个medical语言模型融合，解码器仍然会一个内在的趋势去跟随在合理文本中学习到的语义结构。因此，为了能够适应新领域，深度融合必须首先学习去降低语言中隐含的知识。(意思是说，在融合之前，因为s2s是完整的独立训练的，所以模型自身就会偏向对这些文本结构的判断，所以这种融合在new domain中并不太行)

本文中，我们介绍冷聚合来克服这些局限。冷聚合鼓励s2s在**训练时就使用外部的语言模型**。这意味着s2s能够自然的利用无监督的文本数据，使得它能够在面对新领域的时候很有收益。然后就是在实际中特别重要的就是模型基于训练的领域会和它在使用中的领域不同。在实验中，该模型迁移到另一个领域中会少10倍的训练数据，另外，解码器只需要去学习任务相关的信息，因此训练的更快

# 2 Background and Related work

# 3Cold Fusion

这个想法是由Deep Fusion驱动，但是不同的是，S2s是从零开始和一个固定的预先训练好的模型一起训练的。

因为S2S在训练过程中能够意识到语言模型，所以能使用语言模型来提取特定的信息并且捕捉输入到输出的相关的信息

pass 都没看到模型